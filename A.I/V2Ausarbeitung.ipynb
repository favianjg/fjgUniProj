{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Betrachten Sie das Programmgerüst V2A1_LinearRegression.py aus dem Praktikums- verzeichnis.\n",
    "Erklären Sie kurz in eigenen Worten (jeweils 1-2 Sätze) wozu die Funktionen fun_true(.),\n",
    "generateDataSet(.), getDataError(.) und phi_polynomial(.) dienen.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__fun_true(X):__ die Funktion berechnet y(x,w)= w0 + w1*x + w2*x² (die Linearkomibnation) für jeden Eintrag in X. Die Werte für die Gewichtsvariablen sind in der Funktion hinterlegt.\n",
    "\n",
    "__generateDataSet(N,xmin,xmax,sd_noise):__ generiert die Datenmatrix X, N steht hierbei für die Anzahl der erzeugten Einträge, xmin und xmax geben den Wertebereich der Werte an, die zugehörigen Zierlwerte in T werden mit X und der zuvor erklärten Funktion fun_true(X) erzeugt. \n",
    "\n",
    "__getDataError(Y,T):__ berechnet den Datenfehler zwischen vorhergesagtem Y und generierten Zielwert T - mittels der Least Squares Methode.\n",
    "\n",
    "__phy_polynomial(x,deg=1):__ generiert die Basisfunktionen __phi__(x) = [ 1  x  x²...] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Von welcher Funktion sind die Original-Daten(xn,tn) gesampelt?__ diese werden von der Funktion generateDataSet() gesampelt - welche zur Generierung von tn die Funktion fun_true() nutzt.\n",
    "\n",
    "__Wie lauten die Basisfunktionen φj(x) für j= 1,...,deg des linearen Modells?__ die Basisfunktionen φj(x) lauten wie folgt: phi(x) = [ 1 x x²...x**deg] \n",
    "\n",
    "__Welche Rolle hat die Variable lmbda?__ lmbda steht für den Regulariesierungs-Koeffizient der Least Squares Methode, welche ein Overfitting verhindern soll.\n",
    "\n",
    "__Worin unterscheiden sich die Variablen X, T von X_test, T_test?__ X,T sind Trainingsdaten, X_test,Y_test sind Testdaten (wie der Name schon erahnen lässt)\n",
    "\n",
    "__Was stellen im Plot die grünen Kreuze/Punkte, grüne Kurve, rote Kurve dar?__ grüne Punkte: Testdaten, grüne Kreuze : Lerndaten, grüne Kurve: Funktionskurve, rote Kurve: Regressionskurve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b) Vervollständigen Sie das Programm__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Implementieren Sie die Berechnung der regularisierten Least-Squares-Gewichte W_LSR als M × 1 -Matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHITPHI_lambdaI_inv= np.linalg.inv(np.dot(np.transpose(PHI),PHI)+lmbda*np.eye(M)) # script S.62 \n",
    "W_LSR = np.dot(np.dot(PHITPHI_lambdaI_inv, np.transpose(PHI)),T)    #  REGULARIZED LEAST SQUARES WEIGHTS!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den oberen beiden Code Zeilen wird die Formel W_LSR von Skript S.52 (3.29) ausgeführt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Implementieren Sie die Berechnung der Prognosewerte Y als N × 1 -Matrix.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np.zeros((N,1))\n",
    "Y_learn = np.zeros((N,1))\n",
    "for j in range(N):\n",
    "    Y_test[j][0] = np.sum(np.multiply(np.transpose(W_LSR), [X_test[j][0]**i for i in range(deg+1)]))\n",
    "    Y_learn[j][0] = np.sum(np.multiply(np.transpose(W_LSR), [X[j][0]**i for i in range(deg+1)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Prognosefunktion y(x) berechnet für jeden Wert in X_test (for j in range...) einen Wert aus, indem x * W_LSR_T * phi gerechnet wird (for i in range...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Testen Sie das Programm zunächst ohne Regularisierung ( λ = 0 ) für N = 10:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Welche optimalen Gewichte WLSR erhalten Sie für Polynomgrad 5?__\n",
    "W_LSR= [[ 7.06500519]\n",
    " [-3.86916089]\n",
    " [ 1.16066097]\n",
    " [ 0.27523414]\n",
    " [ 0.23060499]\n",
    " [ 0.02613605]]\n",
    "\n",
    "__Wie groß ist der Lern-Datenfehler ED(WLSR)?__ \n",
    "learn data error =  151.66995610334067\n",
    "\n",
    "__Wie groß ist der Fehler auf den Testdaten?__ \n",
    "test data error =  395.93589328608016\n",
    "\n",
    "__Warum ist der Test-Daten-Fehler größer als der Lern-Daten-Fehler?__\n",
    "Der Lern-Daten-Fehler ist geringer, da auf diesen gefittet - also trainiert - wurde. Da die Test und Lerndaten getrennt wurden, wurde auf die Test-Daten nicht angepasst und der Daten-Fehler ist hier unweigerlich größer, sollte aber für einen guten Algorithmus nicht allzu weit auseinander liegen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vergleichen Sie Lern- und Test-Datenfehler für verschiedene Polynomgrade 1, 2, 3,\n",
    "4, 5, 7, 9?__\n",
    "\n",
    "__1:__\n",
    "learn data error =  1109.2710261079376\n",
    "test data error =  2371.9893767560065\n",
    "\n",
    "\n",
    "__2:__\n",
    "learn data error =  164.25156650010845\n",
    "test data error =  215.7704033395336\n",
    "\n",
    "__3:__\n",
    "learn data error =  163.50151393233\n",
    "test data error =  209.67536254495204\n",
    "\n",
    "__4:__\n",
    "learn data error =  154.65214472845727\n",
    "test data error =  374.25665499982915\n",
    "\n",
    "__5:__\n",
    "learn data error =  151.66995610334067\n",
    "test data error =  395.93589328608016\n",
    "\n",
    "__6:__\n",
    "learn data error =  146.59248883514428\n",
    "test data error =  553.6187092105473\n",
    "\n",
    "__7:__\n",
    "learn data error =  119.13824260532108\n",
    "test data error =  1227.2433837505205\n",
    "\n",
    "__8:__\n",
    "learn data error =  115.63067723946175\n",
    "test data error =  7666.645397197145\n",
    "\n",
    "__9:__\n",
    "learn data error =  6.121381076875562e-11\n",
    "test data error =  37529.46503637711\n",
    "\n",
    "\n",
    "__Welche Phänomene treten bei zu niedrigem bzw. zu hohem Polynomgrad\n",
    "auf? Warum?__\n",
    "\n",
    "Bei zu niedrigen bzw zu hohem Polynomegrad kommt es zum Under- bzw Overfitting. Dadurch weicht der Lerndatenfehler stark vom Testdatenfehler ab, was daran liegt, dass zu sehr(beim Overfitting) auf die Lerndaten, oder zu wenig angepasst wurde, wodurch die Testdaten nicht richtig Klassifiziert werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bestimmen Sie das mittlere Gewicht 1M∑M−1j=0|WLSRj| für verschiedene Polynomgrade 1,2,4,6,9? Warum werden die Gewichte im Mittel größer?__\n",
    "\n",
    "__1:__\n",
    "mean weight =  16.498520760714317\n",
    "\n",
    "__2:__\n",
    "mean weight =  2.8034589676194686\n",
    "\n",
    "__4:__\n",
    "mean weight =  2.645759485825523\n",
    "\n",
    "__6:__\n",
    "mean weight =  3.0791985500657844\n",
    "\n",
    "__9:__\n",
    "mean weight =  33.4307454757652\n",
    "\n",
    "Beim Over- bzw Underfitting nehmen die Gewichte extrem zu, da hier die Anpassung über eine starke Verwendung der Gewichte stattfindet, was zu einer starken Erhöhung der Werte führt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bestimmen Sie den mittleren Lern- bzw. Test-Datenfehler (pro Datenpunkt) für Polynomgrad 9 und verschiedene Größen des Datensets N = 10, 100, 1000, 10000__\n",
    "\n",
    "__mittels getDataError(Y_learn,T)/N__\n",
    "\n",
    "__N=10__\n",
    "\n",
    "learn data error =  6.121381076875562e-12\n",
    "\n",
    "test data error =  3752.9465036377105\n",
    "\n",
    "__N=100__\n",
    "\n",
    "learn data error =  43.33105957908092\n",
    "\n",
    "test data error =  44.07319425722215\n",
    "\n",
    "__N=1000__\n",
    "\n",
    "learn data error =  47.43467339586491\n",
    "\n",
    "test data error =  53.18131707195543\n",
    "\n",
    "__N=10000__\n",
    "\n",
    "learn data error =  50.40076433890366\n",
    "\n",
    "test data error =  50.423607486626125\n",
    "\n",
    "__Warum wird der eine Fehler kleiner und der andere größer?__\n",
    "\n",
    "Der Lerndatenfehler wird größer und der Testdatenfehler kleiner,das liegt daran, dass bei N=10 und deg=9 ein Overfitting auf die Lerndaten stattfindet. Es wird also die Trennung perfekt auf die 10 gegebenen Datenpunkte angepasst, wodurch der Lerndatenfehler kaum existent ist. Die Testdaten sind jedoch kaum angepasst, da die Gerade sehr starke Ausschläge zwischen den Lerndaten hat.\n",
    "Wächst nun N und deg bleibt gleich, so findet kein Overfitting mehr statt, wodurch der Lerndatenfehler steigt, der Testdatenfehler jedoch sinkt, da die Gerade keine extremen Ausschläge mehr macht.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Wieviele Daten N braucht man, damit für Polynomgrad 2 die tatsächlichen Koeffizienten der Original-Funktion fun_true\n",
    "bis auf 10% Genauigkeit geschätzt werden können? (ungefährer Wert reicht)__\n",
    "\n",
    "Um die Genauigkeit der Schätzung bis auf 10% zu bringen sind rausende Datenpunkte nötig.\n",
    "Die tatsächlichen Koeffizienten betragen 2, -1 und 3. \n",
    "Bei N=1000 findet eine erste Annäherung an die 10% Grenze mit den Werten  1.73302014, -0.96907317, 3.00882487]statt. \n",
    "Bei N=5000 befinden sich die Werte (grob) im 10% Bereich der gegebenen Werte mit etwa  2.26, -1.02, 2.99.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Beschreiben Sie kurz (1-2 Sätze) den Nutzen einer Regularisierung?__\n",
    "\n",
    "Regularisierung wird verwendet um eine Überanpassung (overfitting) zu meiden. Große Gewichte, welche typisch hierfür sind, werden nun auch in ihrer \"Länge\" des Gewichtsvektors berücksichtigt und durch einbringen in die Berechnung so \"bestraft\" oder bemerkbar gemacht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bestimmen Sie den Lern- und Test-Datenfehler sowie das mittlere Gewicht für verschiedene Werte λ =0, 0.01, 0.1, 1, 10, 100, 1000, 10000__\n",
    "\n",
    "__0__\n",
    "\n",
    "learn data error =  6.121381076875562e-12\n",
    "test data error =  3752.9465036377105\n",
    "mean weight =  33.4307454757652\n",
    "\n",
    "__0.01__\n",
    "\n",
    "learn data error =  3.405186042439321\n",
    "test data error =  636.4578341801349\n",
    "mean weight =  14.596340243529088\n",
    "\n",
    "__0.1__\n",
    "\n",
    "learn data error =  10.692744753585604\n",
    "test data error =  84.76887178422307\n",
    "mean weight =  3.4180536892402813\n",
    "\n",
    "__1__\n",
    "\n",
    "learn data error =  13.46156767549771\n",
    "test data error =  36.85967945183428\n",
    "mean weight =  0.9811606165361744\n",
    "\n",
    "__10__\n",
    "\n",
    "learn data error =  15.809431539758112\n",
    "test data error =  27.252958759207935\n",
    "mean weight =  0.42991944508130714\n",
    "\n",
    "__100__\n",
    "\n",
    "learn data error =  21.298533792828167\n",
    "test data error =  29.51895773547982\n",
    "mean weight =  0.10356099151824291\n",
    "\n",
    "__1000__\n",
    "\n",
    "learn data error =  24.55459546198786\n",
    "test data error =  33.327554966768574\n",
    "mean weight =  0.03382471662350579\n",
    "\n",
    "__10000__\n",
    "\n",
    "learn data error =  26.002017931414475\n",
    "test data error =  35.52395125454625\n",
    "mean weight =  0.02252536129443288\n",
    "\n",
    "__Welche Probleme treten auf wenn λ zu klein oder zu groß gewählt wird?__\n",
    "\n",
    "Wird λ zu klein gewählt, so wird zu wenig reguliert und es findet immer noch ein Overfitting statt. Wird λ zu groß gewählt so wird zu sehr reguliert und es kommt zu einem Underfitting.\n",
    "\n",
    "__Für welches λ wird der Generalisierungsfehler (auf den Test-Daten) minimal?__\n",
    "\n",
    "Der Generalisierungsfehler ohne λ beträgt etwa 3753. Für λ=1 beträgt der Generalisierungsfehler bereits nur noch etwa 37. λ ist bei 20 mit einem Wert von 26,651 minimal, bei 19 beträgt der Generalisierungsfehler 26,654 und bei 21 beträgt er 26,665.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellcode V2A1_LinearRegression.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2A1_LinearRegression.py \n",
    "# Programmgeruest zu Versuch 2, Aufgabe 1\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fun_true(X):                              # compute 1-dim. parable function; X must be Nx1 data matrix\n",
    "    w2,w1,w0 = 3.0,-1.0,2.0                   # true parameters of parable y(x)=w0+w1*x+w2*x*x\n",
    "    return w0+w1*X+w2*np.multiply(X,X)        # return function values (same size as X)\n",
    "\n",
    "def generateDataSet(N,xmin,xmax,sd_noise):    # generate data matrix X and target values T\n",
    "    X=xmin+np.random.rand(N,1)*(xmax-xmin)    # get random x values uniformly in [xmin;xmax)\n",
    "    T=fun_true(X);                            # target values without noise\n",
    "    if(sd_noise>0):\n",
    "        T=T+np.random.normal(0,sd_noise,X.shape) # add noise \n",
    "    return X,T\n",
    "\n",
    "def getDataError(Y,T):                        # compute data error (least squares) between prediction Y and true target values T\n",
    "    D=np.multiply(Y-T,Y-T);                   # squared differences between Y and T\n",
    "    return 0.5*sum(sum(D));                   # return least-squares data error function E_D\n",
    "\n",
    "def phi_polynomial(x,deg=1):                            # compute polynomial basis function vector phi(x) for data x \n",
    "    assert(np.shape(x)==(1,)), \"currently only 1dim data supported\"\n",
    "    return np.array([x[0]**i for i in range(deg+1)]).T; # returns feature vector phi(x)=[1 x x**2 x**3 ... x**deg]\n",
    "\n",
    "# (I) generate data \n",
    "np.random.seed(10)                            # set seed of random generator (to be able to regenerate data)\n",
    "N=10                                     # number of data samples\n",
    "xmin,xmax=-5.0,5.0                            # x limits\n",
    "sd_noise=10                                   # standard deviation of Guassian noise\n",
    "X,T           = generateDataSet(N, xmin,xmax, sd_noise)             # generate training data\n",
    "X_test,T_test = generateDataSet(N, xmin,xmax, sd_noise)             # generate test data\n",
    "print (\"X=\",X, \"T=\",T)\n",
    "\n",
    "# (II) generate linear least squares model for regression\n",
    "lmbda=10                                                          # no regression\n",
    "deg=9                                                           # degree of polynomial basis functions\n",
    "N,D = np.shape(X)                                                 # shape of data matrix X\n",
    "N,K = np.shape(T)                                                 # shape of target value matrix T\n",
    "PHI = np.array([phi_polynomial(X[i],deg).T for i in range(N)])    # generate design matrix\n",
    "N,M = np.shape(PHI)                                               # shape of design matrix\n",
    "print (\"PHI=\", PHI)\n",
    "PHITPHI_lambdaI_inv= np.linalg.inv(np.dot(np.transpose(PHI),PHI)+lmbda*np.eye(M)) # script S.62 \n",
    "W_LSR = np.dot(np.dot(PHITPHI_lambdaI_inv, np.transpose(PHI)),T)    #  REGULARIZED LEAST SQUARES WEIGHTS!  \n",
    "print (\"W_LSR=\",W_LSR)\n",
    "print (\"X_test\", X_test)\n",
    "\n",
    "# (III) make predictions for test data\n",
    "#PROGNOSIS FOR TEST DATA X_test! (result should be N x 1 matrix, i.e., one prognosis per row)\n",
    "# REPLACE THIS BY PROGNOSIS FOR DATA X! (result should be N x 1 matrix, i.e., one prognosis per row)\n",
    "Y_test = np.zeros((N,1))\n",
    "Y_learn = np.zeros((N,1))\n",
    "for j in range(N):\n",
    "    Y_test[j][0] = np.sum(np.multiply(np.transpose(W_LSR), [X_test[j][0]**i for i in range(deg+1)]))\n",
    "    Y_learn[j][0] = np.sum(np.multiply(np.transpose(W_LSR), [X[j][0]**i for i in range(deg+1)]))\n",
    "\n",
    "print (\"Y_test=\",Y_test)\n",
    "print (\"T_test=\",T_test)\n",
    "print (\"learn data error = \", getDataError(Y_learn,T)/N)\n",
    "print(\"learn data error per point:\",np.multiply(Y_learn-T,Y_learn-T))\n",
    "print (\"test data error = \", getDataError(Y_test,T_test)/N)\n",
    "print(\"test data error per point:\",np.multiply(Y_test-T_test,Y_test-T_test))\n",
    "print (\"W_LSR=\",W_LSR)\n",
    "print (\"mean weight = \", np.mean(np.mean(np.abs(W_LSR))))\n",
    "\n",
    "# (IV) plot data\n",
    "ymin,ymax = -50.0,150.0                     # interval of y data\n",
    "x_=np.arange(xmin,xmax,0.01)                # densely sampled x values\n",
    "Y_LSR = np.array([np.dot(W_LSR.T,np.array([phi_polynomial([x],deg)]).T)[0] for x in x_]);   # least squares prediction\n",
    "Y_true = fun_true(x_).flat\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X.flat,T.flat,c='g',marker='x',s=100)             # plot learning data points (green x)\n",
    "ax.scatter(X_test.flat,T_test.flat,c='g',marker='.',s=100)   # plot test data points (green .)\n",
    "ax.plot(x_,Y_LSR.flat, c='r')         # plot LSR regression curve (red)\n",
    "ax.plot(x_,Y_true, c='g')             # plot true function curve (green)\n",
    "ax.set_xlabel('x')                    # label on x-axis\n",
    "ax.set_ylabel('y')                    # label on y-axis\n",
    "ax.grid()                             # draw a grid\n",
    "plt.ylim((ymin,ymax))                 # set y-limits\n",
    "plt.show()                            # show plot on screen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Versuchen Sie zunächst den Aufbau des Moduls V2A2_Regression.py zu verstehen:__\n",
    "\n",
    "__Betrachten Sie den Aufbau des Moduls durch Eingabe von pydoc V2A2_Regressifier. Welche Klassen gehören zu dem Modul und welchen Zweck haben sie jeweils?__\n",
    "\n",
    "__class Regressifier:__\n",
    "\n",
    "Hierbei handelt es sich um die abstrakte Basisklasse für RegressionsAlgorithmen. Sie stellt das Grundgerüst für konkrete RegressionsAlgorithmen, sowie einige Funktionen, welchen von konkreten Regressionalgorithmen geerbt und Überschrieben werden.\n",
    "\n",
    "__class DataScaler:__\n",
    "\n",
    "Diese Klasse dient zur Vorbereitung und Skalierung von Datenvektoren für die Regressionsalgorithmen. Diese benötigen oft Standardisierte Vektoren und numerische instabilitäten zu vermeiden.\n",
    "\n",
    "__class LSRRegressifier(Regressifier:__\n",
    "\n",
    "Dieses Klasse erbt von der Klasse Regressifier und konkretisiert diese für den Least Squares (oder Maximum Likelihood) Regressifier, diese Klasse verwendet zudem die in Aufgabe 1d betrachtete Regularisierung.\n",
    "\n",
    "__class KNNRegressifier(Regressifier):__ \n",
    "\n",
    "Dieses Klasse erbt von der Klasse Regressifier und konkretisiert diese für den K-nearest_Neighbors ( via KD-Tree ) Regressifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Betrachten Sie nun die Basis-Klasse Regressifier im Quelltext: Wozu dienen jeweils die Methoden fit(self,X,T), predict(self,x) und crossvalidate(self,S,X,T)?__\n",
    "\n",
    "__fit(self,X,T):__\n",
    "\n",
    "Diese Funktion soll die Regressifier mit Trainingsdatenvektor X und zugehörigen Labeln T trainieren. Diese Funktion muss von den konkreten Regressifiern überschrieben und vervollständigt werden.\n",
    "\n",
    "__predict(self,x):__\n",
    "\n",
    "Diese Funktion stellt die Hautpfunktionalität eines Regressfiers dar: sie soll die zugehörige Klasse/das zugehörige Label T zu einem gegebenen Datenvektor x zurückgeben. Auch diese Funktion muss vom konkreten Regressifier überschrieben und vervollständigt werden.\n",
    "\n",
    "__crossvalidate(self,S,X,T):__\n",
    "\n",
    "Führt eine S-Fache Crossvalidation der Daten X mit den zugehörigen Labels T durch. Crossvalidation wurde bereits im letzen Versuch behandelt und wird daher nicht noch einmal genauer erläutert.\n",
    "Diese Funktion returniert Gewichte (Summe der Abweichungen), Standardabweichung (mittelwert der Entfernung zum Zielwert), Minimum (minimale Entfernung zum Zielwert) und Maximum (maximale Entfernung zum Zielwert) der absoluten sowie relativen Fehler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Worin unterscheidet sich crossvalidate(.) von der entsprechenden Methode für Klassifikation (siehe vorigen Versuch)?__\n",
    "\n",
    "Die Krossvalidation der vorigen Aufgabe testete die richtige Zuordnung der Datenpunkte zu Labels. Gemessen wurden hierbei falsche Zuordnungen (nach dem 1-0 Prinzip, entweder es wurde richtig oder falsch zugeordnet) und demnach die Genauigkeit (Accuracy), desweiteren wurde eine Konfusionsmatrix erstellt, welche die Wahrscheinlichkeit angab, dass ein Punkt der KLasse y der KLasse x zugeordnet wurde.\n",
    "\n",
    "Die Krossvalidation dieses Moduls testet für jeden x Wert wie weit der vorhergesagte Wert vom \"wahren\" T-Wert entfernt ist. Zurückgegeben werden dann die Summierten Abweichungen, sowie die größte und kleinste Abweichung etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Betrachten Sie nun die Funktion phi_polynomial(x,deg):__\n",
    "\n",
    "__Was berechnet die Funktion? Welches Ergebnis liefert phi_polynomial([3],5)? Welches Ergebnis liefert phi_polynomial([3,5],2)?__\n",
    "\n",
    "phi_polynomial berechnet die Polynome bis zum n.ten Grad eines gegebenen Vektors. Also bei einem Vektor [x] z.b. [1 x x² x³ ...] bis zum übergebenen Grad n.\n",
    "Bei phi_polynomial([3],5) ist der Datenvektor eindimensional und der Grad 5, dies wäre also [1 x² x³ x^4 x^5] was den Werten [  1   3   9  27  81 243] entspricht.\n",
    "Bei phi_polynomial([3,5], 2) ist der Datenvektor zweidimensional und der Grad 2, dies wäre also [1 x1 x2 x1² x1*x2 x2²]\n",
    "was den Werten [ 1  3  5  9 15 25] entspricht.\n",
    "\n",
    "__Geben Sie eine allgemeine Formel an für das Ergebnis von phi_polynomial([x1,x2],2)__\n",
    "\n",
    "Die allgemeine Formel wäre (wie bereits oben erwähnt):[1 x1 x2 x1² x1*x2 x2²]\n",
    "\n",
    "__Wozu braucht man diese Funktion im Zusammenhang mit Regression?__\n",
    "\n",
    "Diese Funktion berechnet die Basisfunktionen für die Regression. Diese werden für weitere Berechnungen beispielsweise der Designmatrix und im weiteren zur Berechnung der Prediction benötigt. \n",
    "\n",
    "__Bis zu welchem Polynomgrad kann die Funktion Basisfunktionen berechnen?\n",
    "Erweitern Sie die Funktion mindestens bis Grad 5__\n",
    "\n",
    "Die gegebene Funktion kann Basisfunktionen bis zum Polynomgrad 3 berechnen. \n",
    "Die erweiterte Funktion sieht wie folgt aus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# function to compute polynomial basis functions \n",
    "# ----------------------------------------------------------------------------------------- \n",
    "def phi_polynomial(x,deg=1):           # x should be list or np.array or 1xD matrix; returns an 1xM matrix \n",
    "    \"\"\"\n",
    "    polynomial basis function vector; may be used to transform a data vector x into a feature vector phi(x) having polynomial basis function components\n",
    "    :param x: data vector to be transformed into a feature vector\n",
    "    :param deg: degree of polynomial\n",
    "    :returns phi: feature vector \n",
    "    Example: phi_polynomial(x,3) returns for one-dimensional x the vector [1, x, x*x, x*x*x]\n",
    "    \"\"\"\n",
    "    x=np.array(np.mat(x))[0]           # ensure that x is a 1D array (first row of x)\n",
    "    D=len(x)\n",
    "    assert (D==1) or ((D>1) and (deg<=5)), \"phi_polynomial(x,deg) not implemented for D=\"+str(D)+\" and deg=\"+str(deg)    # MODIFY CODE HERE FOR deg>3 !!!!\n",
    "    if(D==1):\n",
    "        phi = np.array([x[0]**i for i in range(deg+1)])\n",
    "    else:\n",
    "        phi = np.array([])\n",
    "        if(deg>=0):\n",
    "            phi = np.concatenate((phi,[1]))      # include degree 0 terms\n",
    "            if(deg>=1): \n",
    "                phi = np.concatenate((phi,x))    # includes degree 1 terms\n",
    "                if(deg>=2):\n",
    "                    for i in range(D):\n",
    "                        phi = np.concatenate(( phi, [x[i]*x[j] for j in range(i+1)] ))    # include degree 2 terms\n",
    "                    if(deg>=3):\n",
    "                        for i in range(D):\n",
    "                            for j in range(i+1):\n",
    "                                phi = np.concatenate(( phi, [x[i]*x[j]*x[k] for k in range(j+1)] ))   # include degree 3 terms\n",
    "                        # EXTEND CODE HERE FOR deg>3!!!!\n",
    "                        if(deg>=4):\n",
    "                            for i in range(D) :\n",
    "                                for j in range(i+1):\n",
    "                                    for k in range(j+1):\n",
    "                                        phi = np.concatenate(( phi, [x[i]*x[j]*x[k]*x[l] for l in range(k+1)] ))\n",
    "                            if(deg>=5):\n",
    "                                for i in range(D) :\n",
    "                                    for j in range(i+1):\n",
    "                                        for k in range(j+1):\n",
    "                                            for l in range(k+1):\n",
    "                                                phi = np.concatenate(( phi, [x[i]*x[j]*x[k]*x[l]*x[m] for m in range(l+1)] ))\n",
    "                                \n",
    "    return phi.T  # return basis function vector (=feature vector corresponding to data vector x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion musste zum einen in der Startabfrage von 3 auf den gewünschten Wert 5 geändert werden, da sonst die Funktion lediglich eine Meldung über einen zu hohen Polynomgrad zurück gibt.\n",
    "Desweiteren muss die Generierungsschleife erweitert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Betrachten Sie die Klasse LSRRegressifier:__\n",
    "\n",
    "__Welche Art von Regressions-Modell berechnet diese Klasse?__\n",
    "\n",
    "Diese Klasse berechnet das Least Squares (= Max Likelihood) Regressions-Modell.\n",
    "\n",
    "__Wozu dienen jeweils die Parameter lmbda, phi, flagSTD und eps?__\n",
    "Der Parameter lmbda steht für die Regulierung, welche ein Overfitting vermeiden soll(siehe 1c), phi enthält die zu einem Datenvektor x gehörenden Basisfunktionen (siehe 2b), flagSTD ist ein Flag welches angbit ob Daten Standardisiert (Funktion DataScaler) werden sollen ( 0 steht für nicht, >0 für soll), eps steht für den maximal tolerierten Abweichungswert für ein gut konditioniertes Problem.\n",
    "\n",
    "__Welche Rolle spielt hier die Klasse DataScaler?__\n",
    "\n",
    "siehe Aufgabe 2a): Diese Klasse dient zur Vorbereitung und Skalierung von Datenvektoren für die Regressionsalgorithmen. Diese benötigen oft Standardisierte Vektoren und numerische instabilitäten zu vermeiden.\n",
    "\n",
    "__In welchen Methoden und zu welchem Zweck werden die Daten ggf. umskaliert?__\n",
    "\n",
    "Das LSR Regressionsmodell verwendet die Klasse DataScaler in den Methoden fit() und predict(), falls das Flag hierfür gesetzt wurde (siehe flagSTD).\n",
    "In der fit Methode werden so die Matrizen für X und T skaliert, \n",
    "In der predict Methode wird der Datenvektor x skaliert und der Zielvektor y wieder zurück skaliert bevor er returniert wird.\n",
    "\n",
    "__Welches Problem kann auftreten wenn man dies nicht tut?__\n",
    "\n",
    "\n",
    "Werden die Daten nicht skaliert so kann es zu Rechenungenauigkeiten kommen, wodurch die WLSR Funktion gegebenenfalls nicht korrekt eingelernt wird und aufgrund von \"schlechter Konditionierung\" einen Fehler wirft und nicht weiter rechnet.\n",
    "\n",
    "\n",
    "__Wozu braucht man die Variablen Z und maxZ in der Methode fit(.)?__\n",
    "\n",
    "Z enthält die Matrix Z:=PHIT_PHI_lmbdaI*PHIT_PHI_lmbdaI_inv-I, diese Matrix sollte eine Null-Matrix ergeben falls das Problem gut konditioniert wurde.\n",
    "In maxZ wird der größte Wert aus der Matrix Z geholt, ist dieser Wert größer, als die in eps definierte tolerierte Abweichung, so wird das Modell als schlecht konditioniert abgebrochen. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vervollständigen Sie die Methoden fit(self,X,T,...) und predict(self,x,...) (vgl. vorige Aufgabe)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self,X,T,lmbda=None,phi=None,flagSTD=None): # train/compute LS regression with data matrix X and target value matrix T\n",
    "        \"\"\"\n",
    "        Train regressifier (see lecture manuscript, theorem 3.11, p33) \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: flagOK: if >0 then all is ok, otherwise matrix inversion was bad conditioned (and results should not be trusted!!!) \n",
    "        \"\"\"\n",
    "        # (i) set parameters\n",
    "        if lmbda==None: lmbda=self.lmbda       # reset regularization coefficient?\n",
    "        if phi==None: phi=self.phi             # reset basis functions?\n",
    "        if flagSTD==None: flagSTD=self.flagSTD # standardize data vectors?\n",
    "        # (ii) scale data for mean=0 and s.d.=0 ?\n",
    "        if flagSTD>0:                          # if yes, then...\n",
    "            self.datascalerX=DataScaler(X)     # create datascaler for data matrix X\n",
    "            self.datascalerT=DataScaler(T)     # create datascaler for target matrix T\n",
    "            X=self.datascalerX.scale(X)        # scale all features (=columns) of data matrix X to mean=0 and s.d.=1\n",
    "            T=self.datascalerT.scale(T)        # ditto for target matrix T\n",
    "        # (iii) compute weight matrix and check numerical condition\n",
    "        flagOK,maxZ=1,0;                       # if <1 then matrix inversion is numerically infeasible\n",
    "        try:\n",
    "            self.N,self.D = X.shape            # data matrix X has size N x D (N is number of data vectors, D is dimension of a vector)\n",
    "            self.M = self.phi(self.D*[0]).size # get number of basis functions  \n",
    "            self.K = T.shape[1]                # DELTE dummy code (just required for dummy code in predict(.): number of output dimensions\n",
    "            PHI = np.array([phi(X[i]) for i in range(self.N)])                   # compute design matrix\n",
    "            PHIT_PHI_lmbdaI = np.dot(np.transpose(PHI),PHI)+lmbda*np.eye(self.M) # compute PHI_T*PHI+lambda*I\n",
    "            PHIT_PHI_lmbdaI_inv = np.linalg.inv(PHIT_PHI_lmbdaI)                 # compute inverse matrix (may be bad conditioned and fail)\n",
    "            self.W_LSR = np.dot(np.dot(PHIT_PHI_lmbdaI_inv, np.transpose(PHI)),T)# compute regularized least squares weights \n",
    "            # (iv) check numerical condition\n",
    "            Z=np.dot(PHIT_PHI_lmbdaI,PHIT_PHI_lmbdaI_inv)-np.eye(lsr.M) #Compute Z:=PHIT_PHI_lmbdaI*PHIT_PHI_lmbdaI_inv-I which should become the zero matrix if good conditioned!\n",
    "            maxZ = abs(Z.max())                # Compute maximum (absolute) componente of matrix Z (should be <eps for good conditioned problem)\n",
    "            assert maxZ<=self.eps              # maxZ should be <eps for good conditioned problems (otherwise the result cannot be trusted!!!)\n",
    "        except: \n",
    "            flagOK=0;\n",
    "            print (\"EXCEPTION DUE TO BAD CONDITION:flagOK=\", flagOK, \" maxZ=\", maxZ)\n",
    "            raise\n",
    "        return flagOK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self,x,flagSTD=None):      # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: predicted target vector y of size K\n",
    "        \"\"\"\n",
    "        if flagSTD==None: flagSTD=self.flagSTD      # standardazion?\n",
    "        if flagSTD>0: x=self.datascalerX.scale(x)   # if yes, then scale x before computing the prediction!\n",
    "        phi_of_x = self.phi(x)                      # compute feature vector phi_of_x for data vector x\n",
    "        y=np.zeros((1,np.shape(x)[0])).T            #compute prediction y for data vector x \n",
    "        for i in range(np.shape(x)[0]):\n",
    "            y[i][0] = np.sum(np.multiply(np.transpose(W_LSR), phi_of_x))\n",
    "        if flagSTD>0: y=self.datascalerT.unscale(y) # scale prediction back to original range?\n",
    "        return y                                    # return prediction y for data vector x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Betrachten Sie die Klasse KNNRegressifier:__\n",
    "\n",
    "__Welche Art von Regressions-Modell berechnet diese Klasse?__\n",
    "\n",
    "Diese Klasse berechnet eine K-Nearest-Neighbours regression ( mit KD-trees, nicht der naiven vorgehensweise vgl. Versuch 1)\n",
    "\n",
    "__Wozu dienen jeweils die Parameter K und flagKLinReg?__\n",
    "\n",
    "K steht für die Anzahl an \"nearest Neighbours\" [nächsten Nachbarn auf Distanz bezogen], welche für die Regression betrachtet werden. FlagKLinReg ist ein Flag, ist dieses gesetzt also >0, so wird mit den k nearest neighbours und deren Zielwerten eine lineare Regression berechnet; ist dieses nicht gesetzt also <=0, so wird lediglich der Mittelwert der Zielwerte der k nearest Neighbours berechnet.\n",
    "\n",
    "__Beschreiben Sie kurz in eigenen Worten (2-3 Sätze) auf welche Weise die Prädiktion y(x) berechnet wird__\n",
    "\n",
    "Die Art auf welche die Prädiktion y(x) berechnet wird, hängt von der Variablen flagKLinReg ab (s.o.). Ist diese <=0 so wird die Prädiktion aus dem Mittelwert der Zielwerte der k-nearest-neighbours berechnet (diese werden wie in Versuch 1 ermittelt). Ist flagKLinReg > = so wird y(x) ermittelt indem die k-nearest-neighbours an das LSR Regressionsmodell, welches in den vorigen Aufgaben erstellt wurde, weitergegeben werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2e "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Betrachten Sie abschließend den Modultest:__\n",
    "\n",
    "__Beschreiben Sie kurz was im Modultest passiert.__\n",
    "\n",
    "Zu Beginn des Modultests werden die nötigen Daten generiert, hierzu gehören zum Beispiel die Gewichtsvariablen, X, T und einige andere. \n",
    "Im nächsten Schritt wird phi (eine Lambda Funktion zur Generierung der Basisfunktionen mittels phi_polynomial) erstellt.\n",
    "\n",
    "Im nächsten Schritt wird eine LSR durchgeführt, hierfür wird ein LSR Regressor erstellt (init), trainiert (fit) und dann eine Prognose für zuvor erstelltes x abgegeben (predict). Diese Funktionen wurden bereits in den vorigen Teilaufgaben beschrieben.\n",
    "Im Anschluss wird noch eine Krossvalidierung des LSR durchgeführt (wurde auch bereits erklärt)\n",
    "\n",
    "Nun wird das gleiche noch für den KNN Regressor durchgeführt: auch dieser wird initialisert (init), trainiert (fit) und eine Prognose berechnet (predict) und im Anschluss krossvalidiert (crossvalidate).\n",
    "\n",
    "__Welche Gewichte W werden gelernt? Wie lautet also die gelernte Prädiktionsfunktion? Welche Funktion sollte sich idealerweise (für N→∞) ergeben?__\n",
    "\n",
    "Die Gewichte W die gelernt werden lauten : lsr.W_LSR= [[3.80928339e+00][1.99127605e+00][4.92339789e-04]]\n",
    "Die gelernte Prädiktionsfunktion lautet damit: w0 +w1x + w2x² = 3.80928339 + 1.99127605x + 4.92339789e-04x²\n",
    "Für N→∞ sollten die Gewichte der gelernten Funktion idealerweise gleich den hinterlegten Gewichtswerten werden. Diese sind im Modultest w0,w1 = 4,2. Daraus ergibt sich im idealfall: w0 +w1x + w2x² = 4 + 2x + 0x².\n",
    "\n",
    "__Welche Ergebnisse liefert die Kreuzvalidierung? Was bedeuten die Werte?__\n",
    "\n",
    "Die Kreuzvalidierung liefert folgende Ergebnisse:\n",
    "LSRRegression cross-validation: absolute errors (E,sd,min,max)= (0.8153947142315253, 0.6513733822330233, 0.0037967168514754235, 2.8092479087788966)   relative errors (E,sd,min,max)= (0.026212655561464408, 0.03957765371996856, 8.427798678956895e-05, 0.21543396375919477)\n",
    "Die Bedeutung dieser Werte wurde bereits in 2a) angeschnitten: \"Diese Funktion returniert Gewichte (Summe der Abweichungen E), Standardabweichung (mittelwert der Entfernung zum Zielwert sd), Minimum (minimale Entfernung zum Zielwert min) und Maximum (maximale Entfernung zum Zielwert max) der absoluten sowie relativen Fehler.\"\n",
    "\n",
    "__Vergleichen und Bewerten Sie die Ergebnisse von Least Squares Regression gegenüber der KNN-Regression (nach Optimierung der Hyper-Parameter λ, K, ...)__\n",
    "\n",
    "Bei LSR sind die Fehlerwerte stets etwas kleiner als bei KNN. Bei KNN gilt es ein geeignetes K zu finden, für welches der Fehler möglichst gering ist, hier ist es also von extremer Wichtigkeit wie viele Datenpunkte zur Verfügung stehen.\n",
    "Bei LSR wird die Optimierung durch eine Anpassung von λ vorgenommen. Dieser Wert ist nicht davon abhänig, wie viele Datenpunkte zur Verfügung stehen. Die Menge der Datenpunkte hat jedoch einen Einfluss darauf, wie Präzise die Gewichte eingelernt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellcode V2A2_Regression.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# V2A2_Regression.py\n",
    "# Programmgeruest zu Versuch 2, Aufgabe 2\n",
    "\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "from random import randint\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "# base class for regressifiers\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class Regressifier:\n",
    "    \"\"\"\n",
    "    Abstract base class for regressifiers\n",
    "    Inherit from this class to implement a concrete regression algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self,X,T):        # train/compute regression with lists of feature vectors X and class labels T\n",
    "        \"\"\"\n",
    "        Train regressifier by training data X, T, should be overwritten by any derived class\n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self,x):      # predict a target vector given the data vector x \n",
    "        \"\"\"\n",
    "        Implementation of the regression algorithm; should be overwritten by any derived class \n",
    "        :param x: test data vector of size D\n",
    "        :returns: predicted target vector\n",
    "        \"\"\"\n",
    "        return None           \n",
    "\n",
    "    def crossvalidate(self,S,X,T,dist=lambda t: np.linalg.norm(t)):  # do a S-fold cross validation \n",
    "        \"\"\"\n",
    "        Do a S-fold cross validation\n",
    "        :param S: Number of parts the data set is divided into\n",
    "        :param X: Data matrix (one data vector per row)\n",
    "        :param T: Matrix of target vectors; T[n] is target vector of X[n]\n",
    "        :param dist: a fuction dist(t) returning the length of vector t (default=Euklidean)\n",
    "        :returns (E_dist,sd_dist,E_min,E_max) : mean, standard deviation, minimum, and maximum of absolute error \n",
    "        :returns (Erel_dist,sdrel_dist,Erel_min,Erel_max) : mean, standard deviation, minimum, and maximum of relative error \n",
    "        \"\"\"\n",
    "        X,T=np.array(X),np.array(T)                         # ensure array type\n",
    "        N=len(X)                                            # N=number of data vectors\n",
    "        perm = np.random.permutation(N)                     # do a random permutation of X and T...\n",
    "        X1,T1=[X[i] for i in perm], [T[i] for i in perm]    # ... to get random partitions of the data set\n",
    "        idxS = [range(i*N//S,(i+1)*N//S) for i in range(S)] # divide data set into S parts:\n",
    "        E_dist,E_dist2,E_max,E_min=0,0,-1,-1                # initialize first two moments of (absolute) regression error as well as max/min error \n",
    "        Erel_dist,Erel_dist2,Erel_max,Erel_min=0,0,-1,-1    # initialize first two moments of relative regression error as well as max/min error \n",
    "        for idxTest in idxS:                                # loop over all possible test data sets\n",
    "            # (i) generate training and testing data sets and train classifier        \n",
    "            idxLearn = [i for i in range(N) if i not in idxTest]                      # remaining indices (not in idxTest) are learning data\n",
    "            if(S<=1): idxLearn=idxTest                                                # if S==1 use entire data set for learning and testing\n",
    "            X_learn, T_learn = np.array([X1[i] for i in idxLearn]), np.array([T1[i] for i in idxLearn]) # learn data \n",
    "            X_test , T_test  = np.array([X1[i] for i in idxTest ]), np.array([T1[i] for i in idxTest ]) # test data \n",
    "            self.fit(X_learn,T_learn)                       # train regressifier\n",
    "            # (ii) test regressifier\n",
    "            for i in range(len(X_test)):  # loop over all data vectors to be tested\n",
    "                # (ii.a) regress for i-th test vector\n",
    "                xn_test = X_test[i].T                           # data vector for testing\n",
    "                t_test = self.predict(xn_test)                  # predict target value for given test vector \n",
    "                # (ii.b) check for regression errors\n",
    "                t_true = T_test[i].T                            # true target value\n",
    "                d=dist(t_test-t_true)                           # (Euklidean) distance between t_test and t_true \n",
    "                dttrue=dist(t_true)                             # length of t_true\n",
    "                E_dist  = E_dist+d                              # sum up distances (for first moment)\n",
    "                E_dist2 = E_dist2+d*d                           # sum up squared distances (for second moment)\n",
    "                if(E_max<0)or(d>E_max): E_max=d                 # collect maximal error\n",
    "                if(E_min<0)or(d<E_min): E_min=d                 # collect minimal error\n",
    "                drel=d/dttrue\n",
    "                Erel_dist  = Erel_dist+drel                     # sum up relative distances (for first moment)\n",
    "                Erel_dist2 = Erel_dist2+(drel*drel)             # sum up squared relative distances (for second moment)\n",
    "                if(Erel_max<0)or(drel>Erel_max): Erel_max=drel  # collect maximal relative error\n",
    "                if(Erel_min<0)or(drel<Erel_min): Erel_min=drel  # collect minimal relative error\n",
    "        E_dist      = E_dist/float(N)                           # estimate of first moment (expected error)\n",
    "        E_dist2     = E_dist2/float(N)                          # estimate of second moment (expected squared error)\n",
    "        Var_dist    = E_dist2-E_dist*E_dist                     # variance of error\n",
    "        sd_dist     = np.sqrt(Var_dist)                         # standard deviation of error\n",
    "        Erel_dist   = Erel_dist/float(N)                        # estimate of first moment (expected error)\n",
    "        Erel_dist2  = Erel_dist2/float(N)                       # estimate of second moment (expected squared error)\n",
    "        Varrel_dist = Erel_dist2-Erel_dist*Erel_dist            # variance of error\n",
    "        sdrel_dist  = np.sqrt(Varrel_dist)                      # standard deviation of error\n",
    "        return (E_dist,sd_dist,E_min,E_max), (Erel_dist,sdrel_dist,Erel_min,Erel_max) # return mean, standard deviation, minimum, \n",
    "                                                                # and maximum error (for absolute and relative distances)  \n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------- \n",
    "# DataScaler: scale data to standardize data distribution (for mean=0, standard deviation =1)  \n",
    "# -------------------------------------------------------------------------------------------- \n",
    "class DataScaler: \n",
    "    \"\"\"\n",
    "    Class for standardizing data vectors \n",
    "    Some regression methods require standardizing of data before training to avoid numerical instabilities!!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,X):               # X is data matrix, where rows are data vectors\n",
    "        \"\"\"\n",
    "        Constructor: Set parameters (mean, std,...) to standardize data matrix X\n",
    "        :param X: Data matrix of size NxD the standardization parameters (mean, std, ...) should be computed for \n",
    "        :returns: object of class DataScaler\n",
    "        \"\"\"\n",
    "        self.meanX = np.mean(X,0)       # mean values for each feature column\n",
    "        self.stdX  = np.std(X,0)        # standard deviation for each feature column \n",
    "        if isinstance(self.stdX,(list,tuple,np.ndarray)): \n",
    "            self.stdX[self.stdX==0]=1.0 # do not scale data with zero std (that is, constant features)\n",
    "        else:\n",
    "            if(self.stdX==0): self.stdX=1.0   # in case stdX is a scalar\n",
    "        self.stdXinv = 1.0/self.stdX    # inverse standard deviation\n",
    "\n",
    "\n",
    "    def scale(self,x):                  # scales data vector x to mean=0 and std=1\n",
    "        \"\"\"\n",
    "        scale data vector (or data matrix) x to mean=0 and s.d.=1 \n",
    "        :param x: data vector or data matrix  \n",
    "        :returns: scaled (standardized) data vector or data matrix \n",
    "        \"\"\"\n",
    "        return np.multiply(x-self.meanX,self.stdXinv)\n",
    "\n",
    "    def unscale(self,x):                # unscale data vector x to original distribution\n",
    "        \"\"\"\n",
    "        unscale data vector (or data matrix) x to original data ranges  \n",
    "        :param x: standardized data vector or data matrix  \n",
    "        :returns: unscaled data vector or data matrix \n",
    "        \"\"\"\n",
    "        return np.multiply(x,self.stdX)+self.meanX\n",
    "\n",
    "    def printState(self):\n",
    "        \"\"\"\n",
    "        print standardization parameters (mean value, standard deviation (std), and inverse of std)  \n",
    "        \"\"\"\n",
    "        print (\"mean=\",self.meanX, \" std=\",self.stdX, \" std_inv=\",self.stdXinv)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "# function to compute polynomial basis functions \n",
    "# ----------------------------------------------------------------------------------------- \n",
    "def phi_polynomial(x,deg=1):           # x should be list or np.array or 1xD matrix; returns an 1xM matrix \n",
    "    \"\"\"\n",
    "    polynomial basis function vector; may be used to transform a data vector x into a feature vector phi(x) having polynomial basis function components\n",
    "    :param x: data vector to be transformed into a feature vector\n",
    "    :param deg: degree of polynomial\n",
    "    :returns phi: feature vector \n",
    "    Example: phi_polynomial(x,3) returns for one-dimensional x the vector [1, x, x*x, x*x*x]\n",
    "    \"\"\"\n",
    "    x=np.array(np.mat(x))[0]           # ensure that x is a 1D array (first row of x)\n",
    "    D=len(x)\n",
    "    assert (D==1) or ((D>1) and (deg<=5)), \"phi_polynomial(x,deg) not implemented for D=\"+str(D)+\" and deg=\"+str(deg)    # MODIFY CODE HERE FOR deg>3 !!!!\n",
    "    if(D==1):\n",
    "        phi = np.array([x[0]**i for i in range(deg+1)])\n",
    "    else:\n",
    "        phi = np.array([])\n",
    "        if(deg>=0):\n",
    "            phi = np.concatenate((phi,[1]))      # include degree 0 terms\n",
    "            if(deg>=1): \n",
    "                phi = np.concatenate((phi,x))    # includes degree 1 terms\n",
    "                if(deg>=2):\n",
    "                    for i in range(D):\n",
    "                        phi = np.concatenate(( phi, [x[i]*x[j] for j in range(i+1)] ))    # include degree 2 terms\n",
    "                    if(deg>=3):\n",
    "                        for i in range(D):\n",
    "                            for j in range(i+1):\n",
    "                                phi = np.concatenate(( phi, [x[i]*x[j]*x[k] for k in range(j+1)] ))   # include degree 3 terms\n",
    "                        # EXTEND CODE HERE FOR deg>3!!!!\n",
    "                        if(deg>=4):\n",
    "                            for i in range(D) :\n",
    "                                for j in range(i+1):\n",
    "                                    for k in range(j+1):\n",
    "                                        phi = np.concatenate(( phi, [x[i]*x[j]*x[k]*x[l] for l in range(k+1)] ))\n",
    "                            if(deg>=5):\n",
    "                                for i in range(D) :\n",
    "                                    for j in range(i+1):\n",
    "                                        for k in range(j+1):\n",
    "                                            for l in range(k+1):\n",
    "                                                phi = np.concatenate(( phi, [x[i]*x[j]*x[k]*x[l]*x[m] for m in range(l+1)] ))\n",
    "                                \n",
    "    return phi.T  # return basis function vector (=feature vector corresponding to data vector x)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# Least Squares (ML) linear regression with sum of squares Regularization,\n",
    "# -----------------------------------------------------------------------------------------\n",
    "class LSRRegressifier(Regressifier):\n",
    "    \"\"\"\n",
    "    Class for Least Squares (or Maximum Likelihood) Linear Regressifier with sum of squares regularization \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lmbda=0,phi=lambda x: phi_polynomial(x,1),flagSTD=0,eps=1e-6):\n",
    "        \"\"\"\n",
    "        Constructor of class LSRegressifier\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :param eps: maximal residual value to tolerate (instead of zero) for numerically good conditioned problems\n",
    "        :returns: -\n",
    "        \n",
    "        \"\"\"\n",
    "        self.lmbda=lmbda       # set regression parameter (default 0)\n",
    "        self.phi=phi           # set basis functions used for linear regression (default: degree 1 polynomials)\n",
    "        self.flagSTD=flagSTD;  # if flag >0 then data will be standardized, i.e., scaled for mean 0 and s.d. 1\n",
    "        self.eps=eps;          # maximal residual value to tolerate (instead of zero) for numerically good conditioned problems\n",
    "\n",
    "\n",
    "    def fit(self,X,T,lmbda=None,phi=None,flagSTD=None): # train/compute LS regression with data matrix X and target value matrix T\n",
    "        \"\"\"\n",
    "        Train regressifier (see lecture manuscript, theorem 3.11, p33) \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: flagOK: if >0 then all is ok, otherwise matrix inversion was bad conditioned (and results should not be trusted!!!) \n",
    "        \"\"\"\n",
    "        # (i) set parameters\n",
    "        if lmbda==None: lmbda=self.lmbda       # reset regularization coefficient?\n",
    "        if phi==None: phi=self.phi             # reset basis functions?\n",
    "        if flagSTD==None: flagSTD=self.flagSTD # standardize data vectors?\n",
    "        # (ii) scale data for mean=0 and s.d.=0 ?\n",
    "        if flagSTD>0:                          # if yes, then...\n",
    "            self.datascalerX=DataScaler(X)     # create datascaler for data matrix X\n",
    "            self.datascalerT=DataScaler(T)     # create datascaler for target matrix T\n",
    "            X=self.datascalerX.scale(X)        # scale all features (=columns) of data matrix X to mean=0 and s.d.=1\n",
    "            T=self.datascalerT.scale(T)        # ditto for target matrix T\n",
    "        # (iii) compute weight matrix and check numerical condition\n",
    "        flagOK,maxZ=1,0;                       # if <1 then matrix inversion is numerically infeasible\n",
    "        try:\n",
    "            self.N,self.D = X.shape            # data matrix X has size N x D (N is number of data vectors, D is dimension of a vector)\n",
    "            self.M = self.phi(self.D*[0]).size # get number of basis functions  \n",
    "            PHI = np.array([phi(X[i]) for i in range(self.N)])                   # compute design matrix\n",
    "            PHIT_PHI_lmbdaI = np.dot(np.transpose(PHI),PHI)+lmbda*np.eye(self.M) # compute PHI_T*PHI+lambda*I\n",
    "            PHIT_PHI_lmbdaI_inv = np.linalg.inv(PHIT_PHI_lmbdaI)                 # compute inverse matrix (may be bad conditioned and fail)\n",
    "            self.W_LSR = np.dot(np.dot(PHIT_PHI_lmbdaI_inv, np.transpose(PHI)),T)# compute regularized least squares weights \n",
    "            # (iv) check numerical condition\n",
    "            Z=np.dot(PHIT_PHI_lmbdaI,PHIT_PHI_lmbdaI_inv)-np.eye(lsr.M)     #Compute Z:=PHIT_PHI_lmbdaI*PHIT_PHI_lmbdaI_inv-I which should become the zero matrix if good conditioned!\n",
    "            maxZ = abs(Z.max())                   # Compute maximum (absolute) componente of matrix Z (should be <eps for good conditioned problem)\n",
    "            assert maxZ<=self.eps              # maxZ should be <eps for good conditioned problems (otherwise the result cannot be trusted!!!)\n",
    "        except: \n",
    "            flagOK=0;\n",
    "            print (\"EXCEPTION DUE TO BAD CONDITION:flagOK=\", flagOK, \" maxZ=\", maxZ)\n",
    "            raise\n",
    "        return flagOK \n",
    "\n",
    "    def predict(self,x,flagSTD=None):      # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: predicted target vector y of size K\n",
    "        \"\"\"\n",
    "        if flagSTD==None: flagSTD=self.flagSTD      # standardazion?\n",
    "        if flagSTD>0: x=self.datascalerX.scale(x)   # if yes, then scale x before computing the prediction!\n",
    "        phi_of_x = self.phi(x)                      # compute feature vector phi_of_x for data vector x\n",
    "        y=np.zeros((1,np.shape(x)[0])).T            #compute prediction y for data vector x \n",
    "        for i in range(np.shape(x)[0]):\n",
    "            y[i][0] = np.sum(np.multiply(np.transpose(W_LSR), phi_of_x))\n",
    "        if flagSTD>0: y=self.datascalerT.unscale(y) # scale prediction back to original range?\n",
    "        return y                                    # return prediction y for data vector x\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# KNN regression \n",
    "# -----------------------------------------------------------------------------------------\n",
    "class KNNRegressifier(Regressifier): \n",
    "    \"\"\"\n",
    "    Class for fast K-Nearest-Neighbor-Regression using KD-trees \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,K,flagKLinReg=0):\n",
    "        \"\"\"\n",
    "        Constructor of class KNNRegressifier\n",
    "        :param K: number of nearest neighbors that are used to compute prediction \n",
    "        :flagKLinReg: if >0 then the do a linear (least squares) regression on the the K nearest neighbors and their target values\n",
    "                      otherwise just take the mean of the K nearest neighbors target vectors\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.K = K                                 # K is number of nearest-neighbors used for majority decision\n",
    "        self.X, self.T = [],[]                     # initially no data is stored\n",
    "        self.flagKLinReg=flagKLinReg               # if flag is set then do a linear regression of the KNN (otherwise just return mean T of the KNN)\n",
    "\n",
    "    def fit(self,X,T): # train/compute regression with lists of data vectors X and target values T\n",
    "        \"\"\"\n",
    "        Train regressifier by stroing X and T and by creating a KD-Tree based on X   \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.X, self.T = np.array(X),np.array(T)   # just store feature vectors X and corresponding class labels T\n",
    "        self.N, self.D = self.X.shape              # store data number N and dimension D\n",
    "        self.kdtree = scipy.spatial.KDTree(self.X) # do an indexing of the feature vectors\n",
    "\n",
    "    def predict(self,x,K=None,flagKLinReg=None):   # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param K: number of nearest neighbors that are used to compute prediction \n",
    "        :flagKLinReg: if >0 then the do a linear (least squares) regression on the the K nearest neighbors and their target values\n",
    "                      otherwise just take the mean of the K nearest neighbors target vectors\n",
    "        :returns: predicted target vector of size K\n",
    "        \"\"\"\n",
    "        if(K==None): K=self.K                      # do a K-NN search...\n",
    "        if(flagKLinReg==None): flagKLinReg=self.flagKLinReg # if flag >0 then do a regression on the K Nearest Neighbors to get the prediction\n",
    "        nn = self.kdtree.query(x,K)                # get indexes of K nearest neighbors of x\n",
    "        if K==1: idxNN=[nn[1]]                     # cast nearest neighbor indexes nn as a list idxNN\n",
    "        else: idxNN=nn[1]\n",
    "        t_out=0\n",
    "        if(self.flagKLinReg==0):\n",
    "            # just take mean value of KNNs\n",
    "            t_out=np.mean([self.T[i] for i in idxNN])\n",
    "        else:\n",
    "            # do a linear regression of the KNNs\n",
    "            lsr=LSRegressifier(lmbda=0.0001,phi=lambda x:phi_polynomial(x,1),flagSTD=1)\n",
    "            lsr.fit(self.X[idxNN],self.T[idxNN])\n",
    "            t_out=lsr.predict(x)\n",
    "        return t_out\n",
    "\n",
    "\n",
    "# *******************************************************\n",
    "# __main___\n",
    "# Module test\n",
    "# *******************************************************\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print (\"\\n-----------------------------------------\")\n",
    "    print (\"Example: 1D-linear regression problem\")\n",
    "    print (\"-----------------------------------------\")\n",
    "    # (i) generate data\n",
    "    N=100\n",
    "    w0,w1=4,2                 # parameters of line\n",
    "    X=np.zeros((N,1))         # x data: allocate Nx1 matrix as numpy ndarray\n",
    "    X[:,0]=np.arange(0,50.0,50.0/N)  # equidistant sampling of the interval [0,50)\n",
    "    T=np.zeros((N,1))         # target values: allocate Nx1 matrix as numpy ndarray\n",
    "    sd_noise = 1.0            # noise power (=standard deviation)\n",
    "    T=T+w1*X+w0 + np.random.normal(0,sd_noise,T.shape)  # generate noisy target values on line y=w0+w1*x\n",
    "    par_lambda = 0            # regularization parameter\n",
    "    print (\"X=\",X)\n",
    "    print (\"T=\",T)\n",
    "\n",
    "    # (ii) define basis functions (phi should return list of basis functions; x should be a list)\n",
    "    deg=2;                               # degree of polynomial\n",
    "    phi=lambda x: phi_polynomial(x,2)    # define phi by polynomial basis-functions up to degree deg \n",
    "    print (\"phi(4)=\", phi([4]))          # print basis function vector [1, x, x*x ...] for x=4\n",
    "    print (\"phi([1,2])=\", phi([1,2]))    # print basis function vector for two-dim. inputs (yields many output components) \n",
    "\n",
    "    # (iii) compute LSR regression\n",
    "    print (\"\\n-----------------------------------------\")\n",
    "    print (\"Do a Least-Squares-Regression\")\n",
    "    print (\"-----------------------------------------\")\n",
    "    lmbda=10;\n",
    "    lsr = LSRRegressifier(lmbda,phi)\n",
    "    lsr.fit(X,T)\n",
    "    print (\"lsr.W_LSR=\",lsr.W_LSR)           # weight vector (should be approximately [w0,w1]=[4,2])\n",
    "    x=np.array([3.1415]).T\n",
    "    print (\"prediction of x=\",x,\"is y=\",lsr.predict(x))\n",
    "\n",
    "    # do S-fold crossvalidation\n",
    "    S=3\n",
    "    err_abs,err_rel = lsr.crossvalidate(S,X,T)\n",
    "    print (\"LSRRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)\n",
    "    # (iv) compute KNN-regression\n",
    "    print (\"\\n-----------------------------------------\")\n",
    "    print (\"Do a KNN-Regression\")\n",
    "    print (\"-----------------------------------------\")\n",
    "    K=5;\n",
    "    knnr = KNNRegressifier(K)\n",
    "    knnr.fit(X,T)\n",
    "    print (\"prediction of x=\",x,\"is y=\",knnr.predict(x))\n",
    "\n",
    "    # do S-fold crossvalidation\n",
    "    err_abs,err_rel = knnr.crossvalidate(S,X,T)\n",
    "    print (\"KNNRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vervollständigen Sie das Programmgerüst V2A3_regression_airfoilnoise.py um eine Least-Squares-Regression auf den Daten zu berechnen. Optimieren Sie die HyperParameter um bei einer S = 3-fachen Kreuzvalidierung möglichst kleine Fehlerwerte zu erhalten.__\n",
    "\n",
    "__Welche Bedeutung haben jeweils die Hyper-Parameter lmbda, deg, flagSTD?__\n",
    "\n",
    "Die Parameter stehen auch hier wieder für den Regularisierungsfaktor (lmbda), den Grad der Basisfunktionen (deg) und dem Flag zur Scalierung (flagSTD) der Daten.\n",
    "\n",
    "__Was passiert ohne Skalierung der Daten (flagSTD =0) bei höheren Polynomgraden(achten Sie auf die Werte vonmaxZ)?__\n",
    "\n",
    "Ohne eine Skalierung der Daten werden die Werte für steigende Polynomgrade neigt ein Computer zu Rechenfehlern, wodurch die Invertierung nicht mehr exakt erfolgen, wodurch auch der Prüfwert zMax nicht mehr sauber berechnet werden kann und das Programm abbricht.\n",
    "\n",
    "__Geben Sie Ihre optimalen Hyper-Parameter sowie die resultierenden Fehler-Werte an__\n",
    "\n",
    "lmbda=25;           # regularization parameter (lambda>0 avoids also singularities)\n",
    "\n",
    "K=5;               # K for K-Nearest Neighbors\n",
    "\n",
    "deg=5;             # degree of basis function polynomials \n",
    "\n",
    "flagSTD=1; \n",
    "\n",
    "\n",
    "absolute errors (E,sd,min,max)= (8.411895013191073, 6.724994754019339, 0.00993119941927302, 39.465709920772625) \n",
    "\n",
    "relative errors (E,sd,min,max)= (0.06761174744849652, 0.05436556342683168, 7.701230977444262e-05, 0.3589390721391586)\n",
    "\n",
    "__Welche Prognosen ergibt Ihr Modell für die neuen Datenvektoren x_test_1=[1250,11,0.2,69.2,0.0051] bzw. x_test_2 [1305,8,0.1,57.7,0.0048]?__\n",
    "\n",
    "Prediction for x_test_1 is y= [[125.72209261]\n",
    " [125.72209261]\n",
    " [125.72209261]\n",
    " [125.72209261]\n",
    " [125.72209261]]\n",
    "\n",
    "Prediction for x_test_2 is y= [[129.19796455]\n",
    " [129.19796455]\n",
    " [129.19796455]\n",
    " [129.19796455]\n",
    " [129.19796455]]\n",
    " \n",
    " __Welchen Polynomgrad und wieviele Basisfunktionen verwendet Ihr Modell?__\n",
    " \n",
    " Das Modell verwendet den Polynomgrad 5 und somit 252 Basisfunktionen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vervollständigen Sie das Programmgerüst V2A3_regression_airfoilnoise.py um eine KNN-Regression auf den Daten zu berechnen. Optimieren Sie die Hyper-Parameter um bei einer S = 3-fachen Kreuzvalidierung möglichst kleine Fehlerwerte zu erhalten.__\n",
    "\n",
    "__Welche Bedeutung haben jeweils die Hyper-Parameter K und flagKLinReg?__\n",
    "\n",
    "K steht für die Anzahl an Nearest-Neighbours welche ermittelt werden sollen. FlagKLinReg ist wieder eine Flag, ist diese gesetzt - also > 0 - so wird eine LinareaRegression mit den zuvor ermittelten K-nearest-neighbours durchgeführt.\n",
    "\n",
    "__Geben Sie Ihre optimalen Hyper-Parameter sowie die resultierenden Fehler-Werte an.__\n",
    "\n",
    "lmbda=25;           # regularization parameter (lambda>0 avoids also singularities)\n",
    "\n",
    "K=15;               # K for K-Nearest Neighbors\n",
    "\n",
    "deg=5;             # degree of basis function polynomials\n",
    "\n",
    "flagKLinReg = 1;   # if flag==1 and K>=D then do a linear regression of the KNNs to make prediction \n",
    "\n",
    "flagSTD=1; \n",
    "\n",
    "IV.3) S= 3 fold Cross Validation:\n",
    "\n",
    "absolute errors (E,sd,min,max)= (4.706459742565466, 3.5446763135397594, 0.006133333333337987, 20.705400000000026) \n",
    "\n",
    "relative errors (E,sd,min,max)= (0.038005132463149065, 0.02876121231887935, 4.749110187103059e-05, 0.14938206584083075)\n",
    "\n",
    "__Welche Prognosen ergibt Ihr Modell für die neuen Datenvektoren x_test_1=[1250,11,0.2,69.2,0.0051] bzw. x_test_2=[1305,8,0.1,57.7,0.0048]?__\n",
    "\n",
    "rediction for x_test_1 is y= 130.28653333333332\n",
    "Prediction for x_test_2 is y= 129.48466666666667\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vergleichen Sie die beiden Modelle. Welches liefert die besseren Ergebnisse?__\n",
    "\n",
    "Der KNN-Regressifier liefter stets die besseren relativen Werte, dies liegt daran, dass dieser bei gesetztem Flag zusätzlich auf die berechneten k-nearest-neighbours noch eine lineareRegression anwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellcode V2A3_regression_airfoilnoise.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# V2A3_regression_airfoilnoise.py\n",
    "# Programmgeruest zu Versuch 2, Aufgabe 3\n",
    "# to log outputs start with: python V2A3_regression_airfoilnoise.py >V2A3_regression_airfoilnoise.log\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from V2A2_Regression import *\n",
    "\n",
    "\n",
    "# ***** MAIN PROGRAM ********\n",
    "# (I) Hyper-Parameters\n",
    "S=3;               # S-fold cross-validation\n",
    "lmbda=25;           # regularization parameter (lambda>0 avoids also singularities)\n",
    "K=15;               # K for K-Nearest Neighbors\n",
    "deg=5;             # degree of basis function polynomials\n",
    "flagKLinReg = 1;   # if flag==1 and K>=D then do a linear regression of the KNNs to make prediction \n",
    "flagSTD=1;         # if >0 then standardize data before training (i.e., scale X to mean value 0 and standard deviation 1)\n",
    "N_pred=5;          # number of predictions on the training set for testing\n",
    "x_test_1 = [1250,11,0.2,69.2,0.0051];   # define test vector 1\n",
    "x_test_2 = [1305,8,0.1,57.7,0.0048];    # define test vector 2\n",
    "\n",
    "# (II) Load data \n",
    "fname='../DATA/AirfoilSelfNoise/airfoil_self_noise.xls'\n",
    "airfoil_data = pd.read_excel(fname,0); # load data as pandas data frame \n",
    "T = airfoil_data.values[:,5]           # target values = noise load (= column 5 of data table)\n",
    "X = airfoil_data.values[:,:5]          # feature vectors (= column 0-4 of data table)\n",
    "N,D=X.shape                            # size and dimensionality of data set\n",
    "idx_perm = np.random.permutation(N)    # get random permutation for selection of test vectors \n",
    "print (\"Data set \",fname,\" has size N=\", N, \" and dimensionality D=\",D)\n",
    "#print (\"X=\",X)\n",
    "#print (\"T=\",T)\n",
    "#print (\"x_test_1=\",x_test_1)\n",
    "#print (\"x_test_2=\",x_test_2)\n",
    "print (\"number of basis functions M=\", len(phi_polynomial(X[1],deg)))\n",
    "\n",
    "# (III) Do least-squares regression with regularization \n",
    "print (\"\\n#### Least Squares Regression with regularization lambda=\", lmbda, \" ####\") \n",
    "# Create and fit Least-Squares Regressifier using polynomial basis function of degree deg and flagSTD for standardization of data  \n",
    "phi=lambda x: phi_polynomial(x,1) \n",
    "lsr = LSRRegressifier(lmbda, phi, flagSTD)\n",
    "lsr.fit(X,T)\n",
    "#print (\"lsr.W_LSR=\",lsr.W_LSR)    # print weight vector for least squares regression  \n",
    "#print (\"III.1) Some predictions on the training data:\")\n",
    "for i in range(N_pred): \n",
    "    n=idx_perm[i]\n",
    "    #print (\"Prediction for X[\",n,\"]=\",X[n],\" is y=\",lsr.predict(X[n]),\", whereas true value is T[\",n,\"]=\",T[n])   # compute prediction for X[n]\n",
    "print (\"III.2) Some predicitions for new test vectors:\")\n",
    "print (\"Prediction for x_test_1 is y=\", lsr.predict(x_test_1))    # compute prediction for x_test_1\n",
    "print (\"Prediction for x_test_2 is y=\", lsr.predict(x_test_2))    # compute prediction for x_test_2\n",
    "print (\"III.3) S=\",S,\"fold Cross Validation:\")\n",
    "err_abs,err_rel = lsr.crossvalidate(S,X,T)                  # do cross validation!! \n",
    "print (\"absolute errors (E,sd,min,max)=\", err_abs, \"\\nrelative errors (E,sd,min,max)=\", err_rel) \n",
    "\n",
    "# (IV) Do KNN regression  \n",
    "print (\"\\n#### KNN regression with flagKLinReg=\", flagKLinReg, \" ####\")\n",
    "knnr = KNNRegressifier(K)                                   #  Create and fit KNNRegressifier\n",
    "knnr.fit(X,T)\n",
    "#print (\"IV.1) Some predictions on the training data:\")\n",
    "for i in range(N_pred): \n",
    "    n=idx_perm[i]\n",
    "    #print (\"Prediction for X[\",n,\"]=\",X[n],\" is y=\",knnr.predict(X[n]),\", whereas true value is T[\",n,\"]=\",T[n])   # compute prediction for X[n]\n",
    "#print (\"IV.2) Some predicitions for new test vectors:\")\n",
    "print (\"Prediction for x_test_1 is y=\", knnr.predict(x_test_1))    # compute prediction for x_test_1\n",
    "print (\"Prediction for x_test_2 is y=\", knnr.predict(x_test_2))    # compute prediction for x_test_2\n",
    "print (\"IV.3) S=\",S,\"fold Cross Validation:\")\n",
    "err_abs,err_rel = knnr.crossvalidate(S,X,T)                   # do cross validation!! \n",
    "print (\"absolute errors (E,sd,min,max)=\", err_abs, \"\\nrelative errors (E,sd,min,max)=\", err_rel) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
