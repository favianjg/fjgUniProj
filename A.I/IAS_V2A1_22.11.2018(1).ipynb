{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Versuch Aufgabe 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe 1a)\n",
    "\n",
    "fun_true : gives the true parabel that will be used as the basis, using the weight vector\n",
    "\n",
    "generatedataset : create learn and test data\n",
    "\n",
    "getdataerror : compute the error value between Y and T\n",
    "\n",
    "phi_polynomial : produce basis function\n",
    "\n",
    "X, T : learn data\n",
    "\n",
    "X_test, T_test : test data\n",
    "\n",
    "Xn, Tn are generated from the minimum xmin, maximum xmax, the noise and the size of matrix N\n",
    "\n",
    "basis function = 1, x1, x2, x1^2, x2^2, x1x2 etc.\n",
    "\n",
    "lmbda = learning rates to minimize error rate // not learning rate but regalarisation factor to avoid overfitting\n",
    "\n",
    "gruene kurve = the optimal Function, the function we try to achieve / make our red line as close to it as possible\n",
    "\n",
    "rote kurve = die lernkurve, or prediction Y that we try to fit to the green curve as much as possible\n",
    "\n",
    "gruenen Kreuze = test data\n",
    "\n",
    "gruenen punkte = learn data\n",
    "\n",
    "//erklärung grüne punkte/kreuze fehlt :P\n",
    "\n",
    "Regularisierung = regulate the change in data/data optimization to achieve closest possible result to the optimal function. Prevents overfitting. Hier we use weight function to try and minimize our error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"X=\",X, \"T=\",T)? (<ipython-input-58-c5ce92b830e2>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-58-c5ce92b830e2>\"\u001b[1;36m, line \u001b[1;32m34\u001b[0m\n\u001b[1;33m    print \"X=\",X, \"T=\",T\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"X=\",X, \"T=\",T)?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# V2A1_LinearRegression.py \n",
    "# Programmgeruest zu Versuch 2, Aufgabe 1\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fun_true(X):                              # compute 1-dim. parable function; X must be Nx1 data matrix\n",
    "    w2,w1,w0 = 3.0,-1.0,2.0                   # true parameters of parable y(x)=w0+w1*x+w2*x*x\n",
    "    return w0+w1*X+w2*np.multiply(X,X)        # return function values (same size as X)\n",
    "\n",
    "def generateDataSet(N,xmin,xmax,sd_noise):    # generate data matrix X and target values T\n",
    "    X=xmin+np.random.rand(N,1)*(xmax-xmin)    # get random x values uniformly in [xmin;xmax)\n",
    "    T=fun_true(X);                            # target values without noise\n",
    "    if(sd_noise>0):\n",
    "        T=T+np.random.normal(0,sd_noise,X.shape) # add noise \n",
    "    return X,T\n",
    "\n",
    "def getDataError(Y,T):                        # compute data error (least squares) between prediction Y and true target values T\n",
    "    D=np.multiply(Y-T,Y-T);                   # squared differences between Y and T\n",
    "    return 0.5*sum(sum(D));                   # return least-squares data error function E_D\n",
    "\n",
    "def phi_polynomial(x,deg=1):                            # compute polynomial basis function vector phi(x) for data x \n",
    "    assert(np.shape(x)==(1,)), \"currently only 1dim data supported\"\n",
    "    return np.array([x[0]**i for i in range(deg+1)]).T; # returns feature vector phi(x)=[1 x x**2 x**3 ... x**deg]\n",
    "\n",
    "# (I) generate data \n",
    "np.random.seed(10)                            # set seed of random generator (to be able to regenerate data)\n",
    "N=100                                          # number of data samples\n",
    "xmin,xmax=-5.0,5.0                            # x limits\n",
    "sd_noise=10                                   # standard deviation of Guassian noise\n",
    "X,T           = generateDataSet(N, xmin,xmax, sd_noise)             # generate training data\n",
    "X_test,T_test = generateDataSet(N, xmin,xmax, sd_noise)             # generate test data\n",
    "print \"X=\",X, \"T=\",T\n",
    "\n",
    "# (II) generate linear least squares model for regression\n",
    "lmbda=0                                                           # no regression\n",
    "deg=2                                                          # degree of polynomial basis functions\n",
    "N,D = np.shape(X)                                                 # shape of data matrix X\n",
    "N,K = np.shape(T)                                                 # shape of target value matrix T\n",
    "PHI = np.array([phi_polynomial(X[i],deg).T for i in range(N)])    # generate design matrix\n",
    "N,M = np.shape(PHI)                                               # shape of design matrix\n",
    "print \"PHI=\", PHI\n",
    "phitphi = np.dot(np.transpose(PHI),PHI)\n",
    "lami = np.dot(lmbda, np.eye(M))\n",
    "add_matrix = phitphi+lami\n",
    "inverse_matrix = np.linalg.inv(add_matrix)\n",
    "phit_t = np.dot(np.transpose(PHI),T)\n",
    "W_LSR = np.dot(inverse_matrix,phit_t) # REPLACE THIS BY REGULARIZED LEAST SQUARES WEIGHTS!  \n",
    "print \"W_LSR=\",W_LSR\n",
    "\n",
    "# (III) make predictions for test data\n",
    "phipol_test = phi_polynomial(X_test)\n",
    "Y_test = np.sum(np.multiply(np.transpose(W_LSR),phipol_test)) # REPLACE THIS BY PROGNOSIS FOR TEST DATA X_test! (result should be N x 1 matrix, i.e., one prognosis per row)\n",
    "Y_learn = np.zeros((N,1))  # REPLACE THIS BY PROGNOSIS FOR TEST DATA X! (result should be N x 1 matrix, i.e., one prognosis per row)\n",
    "print \"Y_test=\",Y_test\n",
    "print \"T_test=\",T_test\n",
    "print \"learn data error = \", getDataError(Y_learn,T)\n",
    "print \"test data error = \", getDataError(Y_test,T_test)\n",
    "print \"W_LSR=\",W_LSR\n",
    "print \"mean weight = \", np.mean(np.mean(np.abs(W_LSR)))\n",
    "\n",
    "# (IV) plot data\n",
    "ymin,ymax = -50.0,150.0                     # interval of y data\n",
    "x_=np.arange(xmin,xmax,0.01)                # densely sampled x values\n",
    "Y_LSR = np.array([np.dot(W_LSR.T,np.array([phi_polynomial([x],deg)]).T)[0] for x in x_]);   # least squares prediction\n",
    "Y_true = fun_true(x_).flat\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X.flat,T.flat,c='g',marker='x',s=100)             # plot learning data points (green x)\n",
    "ax.scatter(X_test.flat,T_test.flat,c='g',marker='.',s=100)   # plot test data points (green .)\n",
    "ax.plot(x_,Y_LSR.flat, c='r')         # plot LSR regression curve (red)\n",
    "ax.plot(x_,Y_true, c='g')             # plot true function curve (green)\n",
    "ax.set_xlabel('x')                    # label on x-axis\n",
    "ax.set_ylabel('y')                    # label on y-axis\n",
    "ax.grid()                             # draw a grid\n",
    "plt.ylim((ymin,ymax))                 # set y-limits\n",
    "plt.show()                            # show plot on screen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_LSR= [[-1.60119806]\n",
      " [-0.08284446]\n",
      " [ 3.56481042]]\n",
      "learn data error =  1109.7629125468982\n",
      "test data error =  1125.2740101587676\n",
      "W_LSR= [[-1.60119806]\n",
      " [-0.08284446]\n",
      " [ 3.56481042]]\n",
      "mean weight =  1.7496176482154688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b550fff978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# V2A1_LinearRegression.py \n",
    "# Programmgeruest zu Versuch 2, Aufgabe 1\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fun_true(X):                              # compute 1-dim. parable function; X must be Nx1 data matrix\n",
    "    w2,w1,w0 = 3.0,-1.0,2.0                   # true parameters of parable y(x)=w0+w1*x+w2*x*x\n",
    "    return w0+w1*X+w2*np.multiply(X,X)        # return function values (same size as X)\n",
    "\n",
    "def generateDataSet(N,xmin,xmax,sd_noise):    # generate data matrix X and target values T\n",
    "    X=xmin+np.random.rand(N,1)*(xmax-xmin)    # get random x values uniformly in [xmin;xmax)\n",
    "    T=fun_true(X);                            # target values without noise\n",
    "    if(sd_noise>0):\n",
    "        T=T+np.random.normal(0,sd_noise,X.shape) # add noise \n",
    "    return X,T\n",
    "\n",
    "def getDataError(Y,T):                        # compute data error (least squares) between prediction Y and true target values T\n",
    "    D=np.multiply(Y-T,Y-T);                   # squared differences between Y and T\n",
    "    return 0.5*sum(sum(D));                   # return least-squares data error function E_D\n",
    "\n",
    "def phi_polynomial(x,deg=1):                            # compute polynomial basis function vector phi(x) for data x \n",
    "    assert(np.shape(x)==(1,)), \"currently only 1dim data supported\"\n",
    "    return np.array([x[0]**i for i in range(deg+1)]).T; # returns feature vector phi(x)=[1 x x**2 x**3 ... x**deg]\n",
    "\n",
    "# (I) generate data \n",
    "np.random.seed(10)                            # set seed of random generator (to be able to regenerate data)\n",
    "N=20                                    # number of data samples\n",
    "xmin,xmax=-5.0,5.0                            # x limits\n",
    "sd_noise=10                                   # standard deviation of Guassian noise\n",
    "X,T           = generateDataSet(N, xmin,xmax, sd_noise)             # generate training data\n",
    "X_test,T_test = generateDataSet(N, xmin,xmax, sd_noise)             # generate test data\n",
    "#print (\"X=\",X, \"T=\",T)\n",
    "\n",
    "# (II) generate linear least squares model for regression\n",
    "lmbda=0  # no regression\n",
    "deg=2                                                             # degree of polynomial basis functions\n",
    "N,D = np.shape(X)                                                 # shape of data matrix X\n",
    "N,K = np.shape(T)                                                 # shape of target value matrix T\n",
    "PHI = np.array([phi_polynomial(X[i],deg).T for i in range(N)])    # generate design matrix\n",
    "N,M = np.shape(PHI)                                               # shape of design matrix\n",
    "#print (\"PHI=\", PHI)\n",
    "#phitphi = np.dot(np.transpose(PHI),PHI)\n",
    "#lami = np.dot(lmbda, np.eye(M))\n",
    "#add_matrix = np.dot(np.transpose(PHI),PHI)+np.dot(lmbda, np.eye(M))\n",
    "inverse_matrix = np.linalg.inv(np.dot(np.transpose(PHI),PHI)+(lmbda*np.eye(M)))\n",
    "phit_t = np.dot(np.transpose(PHI),T)\n",
    "W_LSR = np.dot(inverse_matrix,phit_t) # REPLACE THIS BY REGULARIZED LEAST SQUARES WEIGHTS!  \n",
    "print (\"W_LSR=\",W_LSR)\n",
    "\n",
    "# (III) make predictions for test data\n",
    "phipol_test = [(np.array([X_test[j]**i for i in range(deg+1)]).T) for j in range(len(X_test))]\n",
    "#print(\"PHI_test: \",phipol_test)\n",
    "wlsrt = np.transpose(W_LSR)\n",
    "Y_test = np.zeros((N,1))\n",
    "sumx = [np.sum(np.multiply(wlsrt,phipol_test[i])) for i in range(len(phipol_test))]\n",
    "for i in range(N):\n",
    "    Y_test[i][0]=sumx[i]\n",
    "Y_learn = np.zeros((N,1))\n",
    "phipol_learn = [(np.array([X[j]**i for i in range(deg+1)]).T) for j in range(len(X))]\n",
    "sumx_l = [np.sum(np.multiply(wlsrt,phipol_learn[i])) for i in range(len(phipol_learn))]\n",
    "for i in range(N):\n",
    "    Y_learn[i][0] = sumx_l[i]\n",
    "#print (\"Y_test=\",Y_test)\n",
    "#print (\"T_test=\",T_test)\n",
    "print (\"learn data error = \", getDataError(Y_learn,T))\n",
    "print (\"test data error = \", getDataError(Y_test,T_test))\n",
    "print (\"W_LSR=\",W_LSR)\n",
    "print (\"mean weight = \", np.mean(np.mean(np.abs(W_LSR))))\n",
    "\n",
    "# (IV) plot data\n",
    "ymin,ymax = -50.0,150.0                     # interval of y data\n",
    "x_=np.arange(xmin,xmax,0.01)                # densely sampled x values\n",
    "Y_LSR = np.array([np.dot(W_LSR.T,np.array([phi_polynomial([x],deg)]).T)[0] for x in x_]);   # least squares prediction\n",
    "Y_true = fun_true(x_).flat\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X.flat,T.flat,c='g',marker='x',s=100)             # plot learning data points (green x)\n",
    "ax.scatter(X_test.flat,T_test.flat,c='g',marker='.',s=100)   # plot test data points (green .)\n",
    "ax.plot(x_,Y_LSR.flat, c='r')         # plot LSR regression curve (red)\n",
    "ax.plot(x_,Y_true, c='g')             # plot true function curve (green)\n",
    "ax.set_xlabel('x')                    # label on x-axis\n",
    "ax.set_ylabel('y')                    # label on y-axis\n",
    "ax.grid()                             # draw a grid\n",
    "plt.ylim((ymin,ymax))                 # set y-limits\n",
    "plt.show()                            # show plot on screen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurzes Fazit von mir zu deinem Code:\n",
    "\n",
    "der Code läuft zwar nicht wegen der print() methoden - ich hab jetzt kb die zu ändern, und ich weiß ja dass es bei dir lief, da wir da ja auch zsm mal düber geschaut haben, von daher ist das kein problem\n",
    "\n",
    "an sich finde ich, dass du zu viele Hilfsvariablen verwendest  - aber das ist ja bekanntlich Geschmackssache und die meisten Unternehmen haben für sowas auch Guidelines deswegen merke ich es nur mal an damit du das einfach mal gelesen hast\n",
    "bei dem Zweiten Code teil könntest du die Reihenfolge noch etwas ändern (ich hab es mal unten eingefügt) dann wird es etwas übersichtlicher (wieder meine persönliche Meinung/Geschmack)\n",
    "\n",
    "Dier Erklärungen sind sonst soweit gut :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deine Version:\n",
    "phipol_test = [(np.array([X_test[j]**i for i in range(deg+1)]).T) for j in range(len(X_test))]\n",
    "#print(\"PHI_test: \",phipol_test)\n",
    "Y_test = np.zeros((N,1))\n",
    "sumx = [np.sum(np.multiply(wlsrt,phipol_test[i])) for i in range(len(phipol_test))]\n",
    "for i in range(N):\n",
    "    Y_test[i][0]=sumx[i]\n",
    "Y_learn = np.zeros((N,1))\n",
    "phipol_learn = [(np.array([X[j]**i for i in range(deg+1)]).T) for j in range(len(X))]\n",
    "sumx_l = [np.sum(np.multiply(wlsrt,phipol_learn[i])) for i in range(len(phipol_learn))]\n",
    "for i in range(N):\n",
    "    Y_learn[i][0] = sumx_l[i]\n",
    "    \n",
    "#so fände ich es von der Reihenfolge schöner:\n",
    "Y_test = np.zeros((N,1))\n",
    "Y_learn = np.zeros((N,1))\n",
    "phipol_test = [(np.array([X_test[j]**i for i in range(deg+1)]).T) for j in range(len(X_test))]\n",
    "phipol_learn = [(np.array([X[j]**i for i in range(deg+1)]).T) for j in range(len(X))]\n",
    "sumx = [np.sum(np.multiply(wlsrt,phipol_test[i])) for i in range(len(phipol_test))]\n",
    "sumx_l = [np.sum(np.multiply(wlsrt,phipol_learn[i])) for i in range(len(phipol_learn))]\n",
    "for i in range(N):\n",
    "    Y_test[i][0]=sumx[i]\n",
    "    Y_learn[i][0] = sumx_l[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
